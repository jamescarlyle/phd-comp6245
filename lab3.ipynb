{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec87a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Learning rate.\n",
    "alpha = 0.1\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris ()\n",
    "X = iris.data [:, [2, 3]] # Using petal length and petal width\n",
    "y = iris.target\n",
    "\n",
    "# --- Case 1: Not Linearly Separable ( Versicolor vs. Virginica ) ---\n",
    "X_nls_full = X [ y != 0]\n",
    "y_nls_full = y [ y != 0]\n",
    "y_nls_full [ y_nls_full == 1] = 0 # Versicolor\n",
    "y_nls_full [ y_nls_full == 2] = 1 # Virginica\n",
    "X_train_nls, X_test_nls, y_train_nls, y_test_nls = train_test_split (X_nls_full, y_nls_full, test_size =0.3, random_state =42)\n",
    "\n",
    "# --- Case 2: Linearly Separable ( Setosa vs. The Rest ) ---\n",
    "X_ls_full = X\n",
    "y_ls_full = np.copy ( y )\n",
    "y_ls_full [ y_ls_full != 0] = 1 # Versicolor and Virginica are class 1\n",
    "X_train_ls, X_test_ls, y_train_ls, y_test_ls = train_test_split (X_ls_full, y_ls_full, test_size =0.3, random_state =42)\n",
    "\n",
    "# Helper function to add the bias term (x0 =1)\n",
    "def add_bias ( X ) :\n",
    "    return np.c_ [ np.ones ( X.shape [0]), X ]\n",
    "\n",
    "# Take a scalar or numpy array ‘a‘ and return the sigmoid activation\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "# Compute the Negative Log Likelihood loss\n",
    "def nll_loss(y_true, y_pred):\n",
    "    # NLL is defined as NLL = −[y⋅log(pred)+(1−y)⋅log(1−pred)]\n",
    "    return -np.mean((y_true * np.log(y_pred)) + ((1 - y_true) * np.log(1 - y_pred)))\n",
    "\n",
    "# Calculate gradient of NLL loss with respect to weights W\n",
    "def calculate_gradient(X, y_true, weights):\n",
    "    n = X.shape[0]\n",
    "    # Gradient is deriviative i.e. 1/n . xt . (y^ - y_true) where X is feature matrix, y^ is sigmoid(X . w) and y_true is vector\n",
    "    y_pred = sigmoid(X @ weights)\n",
    "    return (X.T @ (y_pred - y_true)) / n\n",
    "\n",
    "# Perform Gradient Descent. Initialize weights, and then iteratively update them. \n",
    "# The function returns the final weights and a history of the loss and gradient norm.\n",
    "def train_logistic_regression(X, y_true, alpha, iterations):\n",
    "    # Initialise weights.\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    losses = np.zeros(iterations)\n",
    "    gradients = np.zeros(iterations)\n",
    "    X_bias = add_bias(X)\n",
    "    # Iterate\n",
    "    for i in range(iterations):\n",
    "        gradients[i] = calculate_gradient(X_bias, y_true, weights)\n",
    "        losses[i] = nll_loss(y_true, sigmoid(X_bias @ weights))\n",
    "        weights -= alpha * gradients[i]\n",
    "    return weights, gradients, losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c5212",
   "metadata": {},
   "source": [
    "Task 1: Non-Linearly Separable Case Train your model on the X train nls, y train nls dataset. Plot the loss and gradient norm over iterations. You should see them both converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a204919",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights_nls, gradients_nls, losses_nls \u001b[38;5;241m=\u001b[39m train_logistic_regression(X_train_nls, y_train_nls, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(weights_nls)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow\n",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m, in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(X, y_true, alpha, iterations)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Iterate\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 58\u001b[0m     gradients[i] \u001b[38;5;241m=\u001b[39m calculate_gradient(X_bias, y_true, weights)\n\u001b[1;32m     59\u001b[0m     losses[i] \u001b[38;5;241m=\u001b[39m nll_loss(y_true, sigmoid(X_bias \u001b[38;5;241m@\u001b[39m weights))\n\u001b[1;32m     60\u001b[0m     weights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m gradients[i]\n",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m, in \u001b[0;36mcalculate_gradient\u001b[0;34m(X, y_true, weights)\u001b[0m\n\u001b[1;32m     43\u001b[0m n \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Gradient is deriviative i.e. 1/n . xt . (y^ - y_true) where X is feature matrix, y^ is sigmoid(X . w) and y_true is vector\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m sigmoid(X \u001b[38;5;241m@\u001b[39m weights)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m (y_pred \u001b[38;5;241m-\u001b[39m y_true)) \u001b[38;5;241m/\u001b[39m n\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "weights_nls, gradients_nls, losses_nls = train_logistic_regression(X_train_nls, y_train_nls, 0.1, 2000)\n",
    "\n",
    "plt.plot(weights_nls)\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
